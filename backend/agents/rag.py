# agents/rag.py
import os
import json
import requests
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional
from tavily import TavilyClient

from . import AgentBase


class RAGAgent(AgentBase):
    name = "rag"

    def __init__(
        self,
        apikey: str,
        baseurl: str = "https://api.deepseek.com/chat/completions",
        tavily_api_key: Optional[str] = None,
    ):
        self.apikey = apikey
        self.baseurl = baseurl

        self.tavily_api_key = tavily_api_key or os.getenv("TAVILY_API_KEY")
        if not self.tavily_api_key:
            raise ValueError("TAVILY_API_KEY is not set in env or passed explicitly")

        self.tavily = TavilyClient(api_key=self.tavily_api_key)

    # ---------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ----------

    def extract_key_findings(self, ctx: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ describe + —Å–∞–º–æ–≥–æ —Ñ–∞–π–ª–∞."""
        findings: List[str] = []

        # 1) –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ describe (–µ—Å–ª–∏ –µ—Å—Ç—å)
        describe_info = None
        if ctx.get("findings") and isinstance(ctx["findings"], dict):
            describe_info = ctx["findings"].get("describe")

        if isinstance(describe_info, dict):
            shape = describe_info.get("shape")
            if shape:
                findings.append(f"dataset shape: {shape}")
            num_cols = describe_info.get("numeric_columns") or describe_info.get("numericcolumns")
            cat_cols = describe_info.get("categorical_columns") or describe_info.get("categoricalcolumns")
            if num_cols:
                findings.append("numeric metrics: " + ", ".join(num_cols[:5]))
            if cat_cols:
                findings.append("categorical dimensions: " + ", ".join(cat_cols[:5]))

        # 2) –ï—Å–ª–∏ describe –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–ø–∏—Å–∞–ª ‚Äî —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é
        dataset_path = ctx["files"]["dataset"]
        df = pd.read_csv(dataset_path) if dataset_path.endswith(".csv") else pd.read_excel(dataset_path)
        numeric_cols = df.select_dtypes(include="number").columns.tolist()
        cat_cols = df.select_dtypes(exclude="number").columns.tolist()

        if not any("numeric metrics:" in f for f in findings) and numeric_cols:
            findings.append("numeric metrics: " + ", ".join(numeric_cols[:5]))
        if not any("categorical dimensions:" in f for f in findings) and cat_cols:
            findings.append("categorical dimensions: " + ", ".join(cat_cols[:5]))

        # 3) –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç (–±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç)
        if ctx.get("prompt"):
            findings.append("business goal: " + ctx["prompt"][:160])

        return findings[:6]

    def generate_search_queries(self, findings: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 1‚Äì3 –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è Tavily –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Ü–µ–ª–∏."""
        queries: List[str] = []

        for f in findings:
            if "numeric metrics:" in f:
                queries.append("KPI benchmarks for " + f.replace("numeric metrics:", ""))
            elif "categorical dimensions:" in f:
                queries.append("segmentation best practices for " + f.replace("categorical dimensions:", ""))
            elif "business goal:" in f:
                queries.append("data-driven strategies " + f.replace("business goal:", ""))

            if len(queries) >= 3:
                break

        if not queries:
            queries = ["best practices for analysing business tabular datasets"]

        return queries[:3]

    def search_and_retrieve(self, queries: List[str]) -> str:
        """–†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ Tavily –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        contexts: List[str] = []

        for q in queries:
            try:
                ctx = self.tavily.get_search_context(
                    query=q,
                    search_depth="advanced",
                    max_tokens=800,
                )
                contexts.append(f"### Query: {q}\n{ctx}")
            except Exception as e:
                contexts.append(f"### Query: {q}\nTavily error: {e}")

        return "\n\n---\n\n".join(contexts) if contexts else "No external context fetched."

    def build_dataset_summary(self, ctx: Dict[str, Any]) -> str:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ DescribeAgent + –±–∞–∑–æ–≤—É—é info –ø–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É."""
        parts: List[str] = []

        # 1) HTML/—Ç–µ–∫—Å—Ç –∏–∑ describe (–µ—Å–ª–∏ –µ—Å—Ç—å)
        describe_text = None
        if isinstance(ctx.get("insights"), list):
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ insight –æ—Ç describe, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º
            for msg in ctx["insights"]:
                if isinstance(msg, str) and "Telegram-HTML" in msg:
                    describe_text = msg
                    break

        if describe_text:
            parts.append("Dataset description (from DescribeAgent):")
            parts.append(describe_text)

        # 2) –ú–∏–Ω–∏‚Äë—Å–∞–º–º–∞—Ä–∏ –∏–∑ —Ñ–∞–π–ª–∞
        dataset_path = ctx["files"]["dataset"]
        df = pd.read_csv(dataset_path) if dataset_path.endswith(".csv") else pd.read_excel(dataset_path)

        parts.append(f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")
        parts.append("Columns: " + ", ".join(df.columns.tolist()[:15]))

        num_cols = df.select_dtypes(include="number").columns.tolist()
        if num_cols:
            parts.append("Example numeric columns: " + ", ".join(num_cols[:5]))
        cat_cols = df.select_dtypes(exclude="number").columns.tolist()
        if cat_cols:
            parts.append("Example categorical columns: " + ", ".join(cat_cols[:5]))

        return "\n".join(parts)

    def generate_insights_with_rag(
        self,
        dataset_summary: str,
        web_context: str,
        user_prompt: str,
    ) -> str:
        """RAG-–≤—ã–∑–æ–≤ DeepSeek: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ + –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è."""
        system_prompt = """
–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ BI.
–£ —Ç–µ–±—è –µ—Å—Ç—å:
1) –û–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∫–æ–ª–æ–Ω–∫–∏, —Ç–∏–ø—ã, –±–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏).
2) –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ (–∫–µ–π—Å—ã, –±–µ–Ω—á–º–∞—Ä–∫–∏, best practices).

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
- –û–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —Å–º—ã—Å–ª –∫–æ–ª–æ–Ω–æ–∫, –¥–µ–ª–∞—Ç—å —Ä–∞–∑—É–º–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –æ —Å–≤—è–∑—è—Ö,
  –Ω–æ –Ω–µ –≤—ã–¥–∞–≤–∞—Ç—å –∏—Ö –∫–∞–∫ —Å—Ç—Ä–æ–≥–æ –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–µ.
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä—ã, —Ç–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏.
- –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî JSON:

{
  "key_insights": [...],
  "potential_relationships": [...],
  "benchmarks": [...],
  "recommendations": [...],
  "risks_and_caveats": [...],
  "html_summary": "<p>...</p>"
}

–ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ.
"""

        user_content = f"""
–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ü–†–û–ú–ü–¢ (—Ü–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞):
{user_prompt}

–û–ü–ò–°–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê:
{dataset_summary}

–í–ù–ï–®–ù–ò–ï –ò–°–¢–û–ß–ù–ò–ö–ò (Tavily):
{web_context}
"""

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.apikey}",
        }
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": user_content.strip()},
            ],
            "temperature": 0.25,
            "max_tokens": 1800,
        }

        try:
            r = requests.post(self.baseurl, headers=headers, json=payload, timeout=60)
            r.raise_for_status()
            return r.json()["choices"][0]["message"]["content"].strip()
        except Exception as e:
            return json.dumps(
                {
                    "key_insights": [],
                    "potential_relationships": [],
                    "benchmarks": [],
                    "recommendations": [],
                    "risks_and_caveats": [f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RAG-–∏–Ω—Å–∞–π—Ç–æ–≤: {e}"],
                    "html_summary": "<p>RAG-–∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ API.</p>",
                },
                ensure_ascii=False,
                indent=2,
            )

    # ---------- –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –ê–ì–ï–ù–¢–ê ----------

    def run(self, ctx: Dict[str, Any]) -> Dict[str, Any]:
        print("üß† RAGAgent: starting external research with Tavily...")

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –∏ –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã
        findings = self.extract_key_findings(ctx)
        queries = self.generate_search_queries(findings)
        print(f"[RAG] Search queries: {queries}")

        # 2. –†–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫
        web_context = self.search_and_retrieve(queries)

        ctx.setdefault("external_research", {})
        ctx["external_research"]["search_queries"] = queries
        ctx["external_research"]["web_context"] = web_context

        # 3. ¬´–°–∞–º–º–∞—Ä–∏¬ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (–±–µ–∑ StatAgent)
        dataset_summary = self.build_dataset_summary(ctx)

        # 4. RAG-–≤—ã–∑–æ–≤ DeepSeek
        rag_json = self.generate_insights_with_rag(
            dataset_summary=dataset_summary,
            web_context=web_context,
            user_prompt=ctx.get("prompt", ""),
        )

        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        ctx.setdefault("findings", {})
        ctx.setdefault("insights", [])
        ctx.setdefault("metrics", {})

        ctx["findings"]["rag_raw"] = rag_json
        ctx["insights"].append(
            "RAG insights generated from dataset description and external web context."
        )
        ctx["metrics"]["rag_used"] = True

        # 6. –ü–∏—à–µ–º –æ—Ç—á—ë—Ç
        # outdir = Path(ctx["files"]["outdir"])
        files_info = ctx.get("files", {})
        outdir_str = files_info.get("outdir")
        if outdir_str is None:
            # —Ñ–æ–ª–ª–±–µ–∫: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞—Ç–∞–ª–æ–≥, –≥–¥–µ –ª–µ–∂–∏—Ç –¥–∞—Ç–∞—Å–µ—Ç
            dataset_path = Path(files_info["dataset"])
            outdir = dataset_path.parent
        else:
            outdir = Path(outdir_str)
        outdir.mkdir(parents=True, exist_ok=True)
        report_path = outdir / "rag_report.json"
        report_path.write_text(rag_json, encoding="utf-8")

        ctx["files"].setdefault("artifacts", [])
        ctx["files"]["artifacts"].append(str(report_path))
        ctx["brief"] = "RAG: –≤–Ω–µ—à–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞."

        self.save_context(ctx)
        print("‚úÖ RAGAgent: external research completed.")
        return ctx